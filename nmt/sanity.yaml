accum_count:
- 4
accum_steps:
- 0
adagrad_accumulator_init: 0
adam_beta1: 0.9
adam_beta2: '0.998'
alignment_heads: 0
alignment_layer: -3
apex_opt_level: ''
attention_dropout:
- 0.1
average_decay: 0
average_every: 1
avg_tok_max: 20
avg_tok_min: 3
batch_size: '512'
batch_size_multiple: 1
batch_type: tokens
bidir_edges: true
bridge_extra_node: true
bucket_size: 262144
bucket_size_increment: 0
bucket_size_init: -1
cnn_kernel_width: 3
config: .\onmt.fixed256.ascii.yaml
data: '{''corpus_1'': {''path_src'': ''data/train.src'', ''path_tgt'': ''data/train.tgt''},
  ''valid'': {''path_src'': ''data/valid.src'', ''path_tgt'': ''data/valid.tgt''}}'
data_type: text
dec_hid_size: 500
dec_layers: 2
decay_method: noam
decay_steps: 10000
decoder_start_token: <s>
decoder_type: transformer
default_specials:
- <unk>
- <blank>
- <s>
- </s>
doc_length: 200
dropout: '0.3'
dropout_steps:
- 0
early_stopping: '5'
early_stopping_criteria: accuracy
enc_hid_size: 500
enc_layers: 2
encoder_type: transformer
exp: ''
exp_host: ''
feat_merge: concat
feat_vec_exponent: 0.7
feat_vec_size: -1
fuzzy_corpus_ratio: 0.1
fuzzy_threshold: 70
fuzzy_token: "\uFF5Ffuzzy\uFF60"
fuzzymatch_max_length: 70
fuzzymatch_min_length: 4
generator_function: softmax
global_attention: general
global_attention_function: softmax
gpu_backend: nccl
gpu_ranks: []
gpu_verbose_level: 0
group_size: 128
heads: 4
hidden_size: 256
input_feed: 1
insert_ratio: 0.0
isolated_tag: "\uFF5Fph_#_std\uFF60"
keep_checkpoint: -1
label_smoothing: '0.2'
lambda_align: 0.0
lambda_coverage: 0.0
langid: []
layer_norm: standard
layers: 2
learning_rate: '2.0'
learning_rate_decay: 0.5
lm_prior_lambda: 0.0
lm_prior_tau: 1.0
log_file: ''
log_file_level: '0'
lora_alpha: 1
lora_dropout: 0.0
lora_layers: []
lora_rank: 2
loss_scale: 0
mask_length: subword
mask_ratio: 0.0
master_ip: localhost
master_port: 10000
max_context: 1
max_grad_norm: '0'
max_relative_positions: 0
max_tags: 12
model_dtype: fp32
model_task: seq2seq
model_type: text
n_edge_types: 2
n_node: 2
n_sample: 0
n_src_feats: 0
n_steps: 2
norm_eps: 1.0e-06
norm_numbers: true
norm_quote_commas: true
normalization: tokens
num_experts: 0
num_experts_per_tok: 2
num_kv: 0
num_workers: 2
optim: adam
paired_etag: "\uFF5Fph_#_end\uFF60"
paired_stag: "\uFF5Fph_#_beg\uFF60"
parallel_mode: data_parallel
param_init: 0.1
penn: true
permute_sent_ratio: 0.0
poisson_lambda: 3.0
pos_ffn_activation_fn: relu
position_encoding: 'True'
position_encoding_type: SinusoidalInterleaved
post_remove_control_chars: false
pre_replace_unicode_punct: false
prefetch_factor: 200
quant_layers: []
quant_type: ''
random_ratio: 0.0
relative_positions_buckets: 0
replace_length: -1
report_every: 50
reset_optim: none
response_patterns:
- "Response : \uFF5Fnewline\uFF60"
reversible_tokenization: joiner
rnn_size: '256'
rnn_type: LSTM
rotary_dim: 0
rotary_theta: 10000
rotate_ratio: 0.0
save_checkpoint_steps: '250'
save_format: pytorch
save_model: runs/gloss2zh_tiny/model
scripts_nok: []
scripts_ok:
- Latin
- Common
seed: -1
self_attn_type: scaled-dot-flash
share_embeddings: 'True'
share_vocab: 'True'
skip_empty_level: warning
sliding_window: 0
src_delimiter: "\uFF5Ffuzzy\uFF60"
src_ggnn_size: 0
src_lang: ''
src_onmttok_kwargs: '{''mode'': ''none''}'
src_prefix: ''
src_seq_length: 192
src_subword_alpha: 0
src_subword_model: sp_shared.model
src_subword_nbest: 1
src_subword_type: none
src_subword_vocab: ''
src_suffix: ''
src_term_stoken: "\uFF5Fsrc_term_start\uFF60"
src_tgt_ratio: 2
src_vocab: data/src.vocab
src_vocab_size: 32768
src_vocab_threshold: 0
src_word_vec_size: 256
src_words_min_frequency: 0
start_decay_steps: 50000
state_dim: 512
switchout_temperature: 1.0
tags_corpus_ratio: 0.1
tensorboard: 'True'
tensorboard_log_dir: runs/tb_tiny
term_corpus_ratio: 0.3
term_example_ratio: 0.2
term_source_delimiter: "\uFF5Ffuzzy\uFF60"
tgt_lang: ''
tgt_onmttok_kwargs: '{''mode'': ''none''}'
tgt_prefix: ''
tgt_seq_length: 192
tgt_subword_alpha: 0
tgt_subword_model: sp_shared.model
tgt_subword_nbest: 1
tgt_subword_type: none
tgt_subword_vocab: ''
tgt_suffix: ''
tgt_term_etoken: "\uFF5Ftgt_term_end\uFF60"
tgt_term_stoken: "\uFF5Ftgt_term_start\uFF60"
tgt_vocab: data/tgt.vocab
tgt_vocab_size: 32768
tgt_vocab_threshold: 0
tgt_word_vec_size: 256
tgt_words_min_frequency: 0
timeout: 60
tm_delimiter: "\t"
tokendrop_temperature: 1.0
tokenmask_temperature: 1.0
train_from: ''
train_steps: '8000'
transformer_ff: '1024'
transforms:
- sentencepiece
truncated_decoder: 0
upper_corpus_ratio: 0.01
use_ckpting: []
valid_batch_size: 32
valid_metrics: []
valid_steps: '250'
vocab_size_multiple: 8
w_bit: 4
warmup_steps: '4000'
word_vec_size: 256
world_size: '1'
