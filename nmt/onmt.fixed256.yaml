# onmt.fixed256.yaml —— 強制 256 維（CPU、SentencePiece）
transforms: [sentencepiece]
src_subword_model: sp_shared.model
tgt_subword_model: sp_shared.model

data:
  corpus_1:
    path_src: data/train.src
    path_tgt: data/train.tgt
  valid:
    path_src: data/valid.src
    path_tgt: data/valid.tgt

# ===== 核心維度（四個都明寫成 256，避免被預設 500 蓋掉） =====
encoder_type: transformer
decoder_type: transformer
layers: 2
heads: 4                # 4 可以整除 256
rnn_size: 256           # = d_model
word_vec_size: 256
src_word_vec_size: 256
tgt_word_vec_size: 256
transformer_ff: 1024
position_encoding: true
dropout: 0.3
label_smoothing: 0.2

share_vocab: true
share_embeddings: true

optim: adam
adam_beta2: 0.998
decay_method: noam
warmup_steps: 4000
learning_rate: 2.0
max_grad_norm: 0

batch_type: tokens
batch_size: 512
accum_count: [4]
normalization: tokens

train_steps: 8000
valid_steps: 250
save_checkpoint_steps: 250
early_stopping: 5
early_stopping_criteria: accuracy

save_model: runs/gloss2zh_tiny/model
tensorboard: true
tensorboard_log_dir: runs/tb_tiny

# build_vocab 需要這兩個檔名；share_vocab=true 其實只會用到 src.vocab
src_vocab: data/src.vocab
tgt_vocab: data/tgt.vocab

world_size: 1
